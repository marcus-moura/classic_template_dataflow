name: Template dataflow CI/CD

on:
  # Triggers the workflow on push or pull request events but only for the "develop" branch
  push:
    branches: ["main"]
    paths:
      - "classic_templates/**"
      - ".github/workflows/*"
      - "pyproject.toml"
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# Configure env variables
env:
  RUNNER: DataflowRunner
     
# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # Defines the workflow job called "build"
  build-deploy-template:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    env:
      TEMPLATE_SOURCE_PATH: "classic_templates/template_voos/pipe_voos_to_bigquery.py"
      TEMPLATE_NAME: "pipe_voos_to_bigquery"
      TABLE_NAME: "voos_atrasados"
      DATASET_NAME: "raw_curso_dataflow_voos"
      DATA_SOURCE: "voos_sample.csv"
      JOB_NAME: "flow_transform_voos_atrasados"
  
    # Steps will be executed as part of the job
    steps:
      # Clone the repository
      - uses: actions/checkout@v4

      #Runs a single command using the runners shell        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      # Runs a set of commands using the runners shell
      - name: Install dependencies
        run: |
          pip install pipx
          pipx install poetry
          poetry install

      # Performs authentication on Google Cloud with a json credential allocated in secret github
      - id: 'auth'
        name: 'Authenticate to Google Cloud'
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: ${{ secrets.SERVICE_ACCOUNT_JSON }}

      # Configura o Cloud SDK
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      # Runs command gsutil for syns dags
      - name: build template in bucket
        run: |
          poetry run python $TEMPLATE_SOURCE_PATH \
            --project_id ${{ vars.PROJECT }} \
          	--table_name $TABLE_NAME \
          	--dataset_name $DATASET_NAME \
          	--bucket_name ${{ vars.BUCKET_NAME }} \
          	--data_source $DATA_SOURCE \
          	--runner $RUNNER \
          	--project ${{ vars.PROJECT }} \
          	--staging_location gs://${{ vars.BUCKET_NAME }}/staging \
          	--template_location gs://${{ vars.BUCKET_NAME }}/templates/$TEMPLATE_NAME \
          	--region ${{ vars.REGION }} \
          	--save_main_session

      - name: deploy template
        run: |
          gcloud dataflow jobs run $JOB_NAME \
            --gcs-location gs://${{ vars.BUCKET_NAME }}/templates/$TEMPLATE_NAME \
            --parameters table_name=voos_atrasados_2 \
            --region ${{ vars.REGION }}

      - name: wait start job
        run: sleep 60

      - name: Check job status
        id: check_job_status
        run: |
          status=$(gcloud dataflow jobs list --filter="name=$JOB_NAME" --format="value(state)")
          echo "::set-output name=status::$status"
        continue-on-error: true

      - name: Verificar status do job até que seja concluído
        run: |
          while [[ "${{ steps.check_job_status.outputs.status }}" != "JOB_STATE_DONE" ]]; do
            sleep 60
            status=$(gcloud dataflow jobs list --filter="name=$JOB_NAME" --format="value(state)")
            echo "Status do job: $status"
          done

      - name: Imprimir logs do job concluído
        run: |
          gcloud dataflow jobs logs $JOB_NAME --region ${{ vars.REGION }}